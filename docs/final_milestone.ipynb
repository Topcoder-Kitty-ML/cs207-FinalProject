{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS207 Systems Development Final Project: \n",
    "\n",
    "## Automatic Differentiation package\n",
    "## TopCoderKitty-ML\n",
    "\n",
    "**Collaborators**: Tamilyn Chen, Kar-Tong Tan and Mark Lock\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction \n",
    "\n",
    "### Overview\n",
    "\n",
    "Derivatives play an integral role in computational science, ranging from its use in gradient descent, Newton's method, to finding the posteriors of Bayesian models. We discuss numerical differentiation, symbolic differentiation, how both demonstrate limitations, and automatic differentiation, the focus of our software. We acknowledge its effectiveness in both its accuracy and efficiency when evaluating derivatives. Lastly, we provide some real life applications in population dynamics, biology, and politics. \n",
    "\n",
    "### Motivation for Automatic Differentiation\n",
    "\n",
    "Because functions are often too complex to solve analytically, instead, we look to alternative methods that automatically calculate derivatives. There are three main ways to approach this issue: numerical differentiation from finding finite difference approximations, symbolic differentiation through expression manipulation, and automatic differentiation (AD or algorithmic differentiation). While numerical differentiation is easy to code, it is also subject to floating point errors; symbolic differentiation gives exact and accurate results, but is too computationally expensive. Thus, automatic differentiation proves to be the most effective method as it works to resolve both of these issues; AD is both exact/numerically stable and computationally efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "### What is AD?\n",
    "Conceptually straightforward, automatic differentiation can be defined as a family of techniques that evaluate the derivative through the use of elementary arithmetic operations (ie. addition, subtraction, etc.), elementary arithmetic functions (ie. exp, log, sin), and the chain rule. AD is a recursive process that involves repeatedly taking the chain rule to the elementary operations at hand, and allows us to calculate the individual components of the gradient (a list of partial derivatives in terms of each of the inputs) evaluations, to produce results that are automatic and precise. Because AD involves a specific family of techniques that compute derivatives through accumulation of values during code execution to generate numerical derivative evaluations rather than derivative expressions, it can attain machine precision. There are two modes in AD: the forward mode and reverse mode. \n",
    "### Forward Mode\n",
    "The forward mode begins at the innermost portion of the function and repeatedly, or recursively, applies the chain rule while traversing out. Thus, the forward pass creates the evaluation trace, which is a composition of the finite set of elementary operations for which derivatives are known. This is then combined to evaluate the derivative of the overall composition. Notably, the derivative at subsequent steps are calculated based on the derivatives calculated in preceding steps. It is also important to note that the forward pass also finds the derivatives and the values of the partial derivatives at each step, thus requiring the setting of the seed vector, which indicates which input variable to the take the partial derivative in terms of. Taking the derivative of the m dependent output variables in terms of a single independent input variable make up one column of the jacobian matrix. Thus, the full jacobian matrix can be defined as the partial derivative of the m output variables in terms of the n input variables, or applying the forward pass across n evaluations. These recursive steps can be documented in a table, and visually represented through a computational graph.\n",
    "Below is a simple example of the forward mode, which includes the table and graph:\n",
    "\n",
    "The forward mode can be simplified by utilizing another important component of automatic differentiation: dual numbers. Dual numbers are a type of number that uses  and allows for simultaneously automatically differentiating a function while also evaluating the value of the function. \n",
    "The forward mode is efficient and straight- forward because it is able to compute all the derivatives in terms of one input with just one forward pass.\n",
    "### Reverse mode\n",
    "In the backward mode, a forward pass creates the evaluation trace and indicates the partial derivatives at each step, but does not find the values of the partial derivatives. At the end of this process, the final node’s derivative is evaluated by using an arbitrary seed. Then, the values of the partial derivatives that constitute the end node’s derivative are found by performing a backward pass through the tree to get all the values. \n",
    "During both the forward and backward modes, all intermediate variables are evaluated, and their values are stored; these steps can be represented in a table, and further visualized in a computational graph. The graph (and table) essentially outlines this repeated chain rule process; it also serves as the basis of the logic behind our automatic differentiation software library.\n",
    "\n",
    "### Jacobian product\n",
    "Auto differentiation can take in a vector of input functions with multiple variables. In this case, the autodifferentiation algorithm returns the Jacobian Product matrix which is just a matrix where each row represents the partial derivatives of the variables of a function within the vector. If the vector has m functions then the Jacobian product will have m rows. If the functions contain n variables, the Jacobian will contain n columns. The Jacobian product matrix is handy in first order derivative applications in the sciences when you are dealing with lots of different systems (functions) and unkowns in the systems (variables). We will see an application of this in the RNA velocity package we will be building. \n",
    "\n",
    "### Application of AD\n",
    "AD can be applied to many branches of computational science, ranging in areas from biology to politics. We chose to focus our application in a biological context, namely RNA velocity. This involves a system of complicated nonlinear differential equations that is able to predicts the future state of individual cells. Estimating this is important in aiding the analysis of developmental lineages and cellular dynamics, particularly in humans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "This library provides an easy way to automatically differentiate arbritrarily complex expressions that consist of these elementary functions.\n",
    "\n",
    "- sin\n",
    "- cos\n",
    "- tan\n",
    "- exp\n",
    "- powers\n",
    "- multiplication\n",
    "- division\n",
    "- addition\n",
    "- subtraction\n",
    "- negation\n",
    "\n",
    "How this library differentiates is using forward mode automatic differentiation.\n",
    "\n",
    "To do this one can instantiate autodiff classes, which hold a value and a first order derivative value. We can combine them using operations to arrive at derivative evaluations. \n",
    "\n",
    "Note: this version only intakes single functions with single variable inputs. The final product will intake vectors of functions with multiple inputs to return jacobian product matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting setup\n",
    "\n",
    "Installing the library is easy. Just clone the repository using:\n",
    "\n",
    "git clone https://github.com/Topcoder-Kitty-ML/cs207-FinalProject.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a virtual environment from the requirements.txt file. Using the following command:\n",
    "    \n",
    "1. cd to the directory where requirements.txt is located\n",
    "2. activate your virtualenv (see how to do so here according to your OS: https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/26/python-virtual-env/)\n",
    "3. run: ```pip install -r requirements.txt``` in your shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tamilynchen/Documents/cs207-FinalProject/docs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tamilynchen/Documents/cs207-FinalProject\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genericdiff import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods and examples\n",
    "\n",
    "The way which the package can differentiate automatically is through the instantiation of an GenericDiff object.\n",
    "\n",
    "The GenericDiff object takes an input value. \n",
    "\n",
    "It can then be modified and combined with whatever elemental functions / operations listed above. At each modification, the value of the object and the derivative of the object is updated. These are attributes of the objects which can be accessed.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x is: 1\n",
      "The derivative of x is: 2\n"
     ]
    }
   ],
   "source": [
    "val = 1\n",
    "der = 2\n",
    "x = GenericDiff(val, der)\n",
    "\n",
    "print(\"The value of x is:\", x.val)\n",
    "print(\"The derivative of x is:\", x.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then combine two objects together and get their values and derivatives using the mathematical operators:\n",
    "\n",
    "1. subtraction -\n",
    "2. addition +\n",
    "3. division /\n",
    "4. multiplication *\n",
    "5. power **\n",
    "6. unary negation -\n",
    "\n",
    "Here we demo the + operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x + 2 is: 3\n",
      "The derivative of x + 2 is: 4\n"
     ]
    }
   ],
   "source": [
    "val = 1 \n",
    "der = 4\n",
    "const = 2\n",
    "\n",
    "x = GenericDiff(val, der)\n",
    "\n",
    "h = x + const\n",
    "print(\"The value of x + 2 is:\", h.val)\n",
    "print(\"The derivative of x + 2 is:\", h.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demo the power operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x ** 2 is: 1\n",
      "The derivative of x ** 2 is: 8.0\n"
     ]
    }
   ],
   "source": [
    "val = 1 \n",
    "der = 4\n",
    "const = 2\n",
    "\n",
    "x = GenericDiff(val, der)\n",
    "\n",
    "g = x ** const\n",
    "\n",
    "print(\"The value of x ** 2 is:\", g.val)\n",
    "print(\"The derivative of x ** 2 is:\", g.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply trigonometric and exp functions to GenericDiff objects.\n",
    "\n",
    "The available functions are:\n",
    "\n",
    "1. sine ```sin(x)```\n",
    "2. cosine ```cos(x)```\n",
    "3. tan ```tan(x)```\n",
    "4. $e^x$ ```exponential(x)```\n",
    "\n",
    "Here we demo the sine function by applying it to the x + 2 function we created above:\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of sin(x+2) is: 0.1411200080598672\n",
      "The derivative of sin(x+2) is: -3.9599699864017817\n"
     ]
    }
   ],
   "source": [
    "j = sin(h)\n",
    "\n",
    "print(\"The value of sin(x+2) is:\", j.val)\n",
    "print(\"The derivative of sin(x+2) is:\", j.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the \"jacobian product\" in this case is ```j.der```. It is a scalar value since we are only doing single input single functions in this iteration. In the next iteration we will have a class that intakes a vector of autodiff objects representing functions with multiple inputs.\n",
    "\n",
    "We can further complicate this by exponentiating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of exp(sin(x+2)) is: 1.151562836514535\n",
      "The derivative of exp(sin(x+2)) is: -4.5601542700532605\n"
     ]
    }
   ],
   "source": [
    "g = exp(j)\n",
    "\n",
    "print(\"The value of exp(sin(x+2)) is:\", g.val)\n",
    "print(\"The derivative of exp(sin(x+2)) is:\", g.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also demo our JacobianProduct class by instantiating the JacobianProduct object and apply the partial and jacobian_product methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.0, 4.0, 6.0], [3.0, 12.0, 27.0]]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x, y: x**2 - y**3\n",
    "h = lambda x, y: x**3 + y**3\n",
    "function_vector = [f, h]\n",
    "\n",
    "jp_object = JacobianProduct(function_vector)\n",
    "inputs = [[1, 2, 3], 0]\n",
    "\n",
    "# getting partial with respect to x (position 0 in lambdas)\n",
    "partial_wrt_x = jp_object.partial(wrt=0, inputs=inputs)\n",
    "print(partial_wrt_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 2., -3.],\n",
      "       [ 3.,  3.]]), array([[  4., -12.],\n",
      "       [ 12.,  12.]]), array([[  6., -27.],\n",
      "       [ 27.,  27.]])]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x, y: x**2 - y**3\n",
    "h = lambda x, y: x**3 + y**3\n",
    "function_vector = [f, h]\n",
    "jp_object = JacobianProduct(function_vector)\n",
    "# inputs are x = {1, 2, 3} and y = {1, 2, 3}\n",
    "# this means we will calculate derivatives for\n",
    "# (1, 1), (2, 2), and (3, 3)\n",
    "inputs = [[1, 2, 3], [1, 2, 3]]\n",
    "\n",
    "# getting jp matrix with respect to all variables\n",
    "jp_matrix = jp_object.jacobian_product(inputs)\n",
    "matrix_1 = np.array([[2,-3],[3, 3]])\n",
    "matrix_2 = np.array([[4, -12],[12, 12]])\n",
    "matrix_3 = np.array([[6, -27],[27, 27]])\n",
    "print(jp_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software organization\n",
    "\n",
    "Our implentation philosphy was to create an object such that all the elemental functions and constants could be arbritrarily combined and nested for differentiation.\n",
    "\n",
    "We created an AD library that contains the following modules:\n",
    "- ```__init__.py``` : Initializes the AD package \n",
    "\n",
    "\n",
    "- ```generic_diff.py``` : Contains GenericDiff class where any specialized classes like a linear, sine, or cos class were reduced to just value and derivative attributes such that we could use our overloaded operators. +, -, /, * , **, and negation were defined according to to the following differentiation rules: chain rule, generalized power rule, quotient rule and product rule. It also overloaded comparison operators, which include <, >, <=, >=, ==, and !=.\n",
    "\n",
    "\n",
    "- ```elemental_functions.py```: Specialized classes in the following modules that inherited from GenericDiff- exponential, sin, cos, tan, sinh, cosh, tanh, acos, asin, atan, exp, log, logit, sqrt classes were defined here. \n",
    "\n",
    "\n",
    "- ```vector_jacobian.py```: A class to handle taking in a vector of autodiff objects and producing a jacobian product matrix\n",
    "\n",
    "We created the following to our generic_diff, elemental_functions, and vector_jacobian modules for all methods and error handling:\n",
    "\n",
    "- ```test_elemental_functions.py```\n",
    "\n",
    "- ```test_generic_diff.py```\n",
    "\n",
    "- ```test_generic_diff_comparisons.py```\n",
    "\n",
    "- ```test_vector_jacobian.py```\n",
    "\n",
    "The test suite is run through pytest and called through Travis CI using the travis yml file - it sends a report to codecov for code coverage.\n",
    "\n",
    "The driver script is:\n",
    "\n",
    "- ```driver_root_solving.py``` This solves for roots using our package according to newton's method\n",
    "\n",
    "Installation:\n",
    "\n",
    "In this iteration, the user is given instructions to download the library through github and create a virtual environment directly. In the final iteration we will be building the package and uploading to Anaconda cloud for distribution. How this is done is by first installing the anaconda client in a local machine, building the package using ```conda build``` from files cloned from the repository and then uploading to an anaconda path defined by our team's anaconda login. We plan to follow the instructions here: https://docs.anaconda.com/anaconda-cloud/user-guide/tasks/work-with-packages/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
