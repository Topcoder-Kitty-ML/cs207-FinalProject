{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS207 Systems Development Final Project: \n",
    "\n",
    "## Automatic Differentiation package: genericdiff\n",
    "## TopCoderKitty-ML\n",
    "\n",
    "**Collaborators**: Tamilyn Chen, Kar-Tong Tan and Mark Lock\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction \n",
    "\n",
    "### Overview\n",
    "\n",
    "Derivatives play an integral role in computational science, ranging from its use in gradient descent, Newton's method, to finding the posteriors of Bayesian models. We discuss numerical differentiation, symbolic differentiation, how both demonstrate limitations, and automatic differentiation, the focus of our software. We acknowledge its effectiveness in both its accuracy and efficiency when evaluating derivatives. Lastly, we provide real life applications in the biological context. \n",
    "\n",
    "### Motivation for Automatic Differentiation\n",
    "\n",
    "Because functions are often too complex to solve analytically, instead, we look to alternative methods that automatically calculate derivatives. There are three main ways to approach this issue: numerical differentiation from finding finite difference approximations, symbolic differentiation through expression manipulation, and automatic differentiation (AD or algorithmic differentiation). While numerical differentiation is easy to code, it is also subject to floating point errors; symbolic differentiation gives exact and accurate results, but is too computationally expensive. Thus, automatic differentiation proves to be the most effective method as it works to resolve both of these issues; AD is both exact/numerically stable and computationally efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "### What is AD?\n",
    "Conceptually straightforward, automatic differentiation can be defined as a family of techniques that evaluate the derivative through the use of elementary arithmetic operations (ie. addition, subtraction, etc.), elementary arithmetic functions (ie. exp, log, sin), and the chain rule. AD is a recursive process that involves repeatedly taking the chain rule to the elementary operations at hand, and allows us to calculate the individual components of the gradient (a list of partial derivatives in terms of each of the inputs) evaluations, to produce results that are automatic and precise. Because AD involves a specific family of techniques that compute derivatives through accumulation of values during code execution to generate numerical derivative evaluations rather than derivative expressions, it can attain machine precision. There are two modes in AD: the forward mode and reverse mode. \n",
    "### Forward Mode\n",
    "The forward mode begins at the innermost portion of the function and repeatedly, or recursively, applies the chain rule while traversing out. Thus, the forward pass creates the evaluation trace, which is a composition of the finite set of elementary operations for which derivatives are known. This is then combined to evaluate the derivative of the overall composition. Notably, the derivative at subsequent steps are calculated based on the derivatives calculated in preceding steps. It is also important to note that the forward pass also finds the derivatives and the values of the partial derivatives at each step, thus requiring the setting of the seed vector, which indicates which input variable to the take the partial derivative in terms of. Taking the derivative of the m dependent output variables in terms of a single independent input variable make up one column of the jacobian matrix. Thus, the full jacobian matrix can be defined as the partial derivative of the m output variables in terms of the n input variables, or applying the forward pass across n evaluations. These recursive steps can be documented in a table, and visually represented through a computational graph.\n",
    "Below is a simple example of the forward mode, which includes the table and graph:\n",
    "\n",
    "The forward mode can be simplified by utilizing another important component of automatic differentiation: dual numbers. Dual numbers are a type of number that uses  and allows for simultaneously automatically differentiating a function while also evaluating the value of the function. \n",
    "The forward mode is efficient and straight- forward because it is able to compute all the derivatives in terms of one input with just one forward pass.\n",
    "### Reverse mode\n",
    "In the backward mode, a forward pass creates the evaluation trace and indicates the partial derivatives at each step, but does not find the values of the partial derivatives. At the end of this process, the final node’s derivative is evaluated by using an arbitrary seed. Then, the values of the partial derivatives that constitute the end node’s derivative are found by performing a backward pass through the tree to get all the values. \n",
    "During both the forward and backward modes, all intermediate variables are evaluated, and their values are stored; these steps can be represented in a table, and further visualized in a computational graph. The graph (and table) essentially outlines this repeated chain rule process; it also serves as the basis of the logic behind our automatic differentiation software library.\n",
    "\n",
    "### Jacobian product\n",
    "Auto differentiation can take in a vector of input functions with multiple variables. In this case, the autodifferentiation algorithm returns the Jacobian Product matrix which is just a matrix where each row represents the partial derivatives of the variables of a function within the vector. If the vector has m functions then the Jacobian product will have m rows. If the functions contain n variables, the Jacobian will contain n columns. The Jacobian product matrix is handy in first order derivative applications in the sciences when you are dealing with lots of different systems (functions) and unkowns in the systems (variables). We will see an application of this in the RNA velocity package we will be building. \n",
    "\n",
    "### Application of AD\n",
    "AD can be applied to many branches of computational science, ranging from areas in biology to politics. We chose to focus our application in a biological context, namely RNA velocity. This involves a system of complicated nonlinear differential equations that is able to predict the future state of individual cells. Estimating this is important in aiding the analysis of developmental lineages and cellular dynamics, particularly in humans. We will go into more detail of this extension below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "This library provides an easy way to automatically differentiate arbritrarily complex expressions that consist of these elementary functions.\n",
    "\n",
    "- sin\n",
    "- cos\n",
    "- tan\n",
    "- sinh\n",
    "- cosh\n",
    "- tanh\n",
    "- asin\n",
    "- acos\n",
    "- atan\n",
    "- log\n",
    "- logit\n",
    "- sqrt\n",
    "- exp\n",
    "- powers\n",
    "- multiplication\n",
    "- division\n",
    "- addition\n",
    "- subtraction\n",
    "- negation\n",
    "\n",
    "How this library differentiates is using forward mode automatic differentiation.\n",
    "\n",
    "To do this one can instantiate the GenericDiff class, which hold a value and a first order derivative value. We can combine them using operations to arrive at derivative evaluations. \n",
    "\n",
    "This intakes vectors of functions with multiple inputs to return jacobian product matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting setup\n",
    "\n",
    "Installing the library is easy. Just clone the repository using:\n",
    "\n",
    "git clone https://github.com/Topcoder-Kitty-ML/cs207-FinalProject.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a virtual environment from the requirements.txt file. Using the following command:\n",
    "    \n",
    "1. cd to the directory where requirements.txt is located\n",
    "2. activate your virtualenv (see how to do so here according to your OS: https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/26/python-virtual-env/)\n",
    "3. run: ```pip install -r requirements.txt``` in your shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tamilynchen/Documents/cs207-FinalProject\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genericdiff import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods and examples\n",
    "\n",
    "The way which the package can differentiate automatically is through the instantiation of an GenericDiff object.\n",
    "\n",
    "The GenericDiff object takes two input values, one for the chosen value, and one for its derivative. \n",
    "\n",
    "It can then be modified and combined with whatever elemental functions / operations listed above. At each modification, the value of the object and the derivative of the object is updated. These are attributes of the objects which can be accessed.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x is: 1\n",
      "The derivative of x is: 2\n"
     ]
    }
   ],
   "source": [
    "val = 1\n",
    "der = 2\n",
    "x = GenericDiff(val, der)\n",
    "\n",
    "print(\"The value of x is:\", x.val)\n",
    "print(\"The derivative of x is:\", x.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then combine two objects together and get their values and derivatives using the mathematical operators:\n",
    "\n",
    "1. subtraction -\n",
    "2. addition +\n",
    "3. division /\n",
    "4. multiplication *\n",
    "5. power **\n",
    "6. unary negation -\n",
    "\n",
    "Here we demo the + operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x + 2 is: 3\n",
      "The derivative of x + 2 is: 4\n"
     ]
    }
   ],
   "source": [
    "val = 1 \n",
    "der = 4\n",
    "const = 2\n",
    "\n",
    "x = GenericDiff(val, der)\n",
    "\n",
    "h = x + const\n",
    "print(\"The value of x + 2 is:\", h.val)\n",
    "print(\"The derivative of x + 2 is:\", h.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demo the power operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x ** 2 is: 1\n",
      "The derivative of x ** 2 is: 8.0\n"
     ]
    }
   ],
   "source": [
    "val = 1 \n",
    "der = 4\n",
    "const = 2\n",
    "\n",
    "x = GenericDiff(val, der)\n",
    "\n",
    "g = x ** const\n",
    "\n",
    "print(\"The value of x ** 2 is:\", g.val)\n",
    "print(\"The derivative of x ** 2 is:\", g.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply trigonometric and exp functions to GenericDiff objects.\n",
    "\n",
    "The available functions are:\n",
    "\n",
    "1. sine    ```sin(x)```\n",
    "2. cosine  ```cos(x)```\n",
    "3. tangent ```tan(x)```\n",
    "4. $e^x$   ```exponential(x)```\n",
    "5. hyperbolic sine ```sinh(x)```\n",
    "6. hyperbolic cosine ```cosh(x)```\n",
    "7. hyperbolic tangent ```tanh(x)```\n",
    "8. arc sine  ```asin(x)```\n",
    "9. arc cosine ```acos(x)```\n",
    "10. arc tangent ```atan(x)```\n",
    "11. log ```log(x)```\n",
    "12. logit ```logit(x)```\n",
    "13. square root ```sqrt(x)```\n",
    "\n",
    "\n",
    "Here we demo the sinh function by applying it to the x + 2 function we created above:\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of sinh(x+2) is: 10.017874927409903\n",
      "The derivative of sinh(x+2) is: 40.27064798311106\n"
     ]
    }
   ],
   "source": [
    "j = sinh(h)\n",
    "\n",
    "print(\"The value of sinh(x+2) is:\", j.val)\n",
    "print(\"The derivative of sinh(x+2) is:\", j.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the \"jacobian product\" in this case is ```j.der```. It is a scalar value since we are only doing single input single functions in this iteration. \n",
    "\n",
    "We can further complicate this by exponentiating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of exp(sinh(x+2)) is: 22423.727203951148\n",
      "The derivative of exp(sinh(x+2)) is: 903018.0246996279\n"
     ]
    }
   ],
   "source": [
    "g = exp(j)\n",
    "\n",
    "print(\"The value of exp(sinh(x+2)) is:\", g.val)\n",
    "print(\"The derivative of exp(sinh(x+2)) is:\", g.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we demo the comparison operators, == and !=, where we compare the derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(j.der == g.der)\n",
    "print(j.der != g.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also demo our JacobianProduct class by instantiating the JacobianProduct object and apply the partial method, which gets the partial derivative with respect to x, as well as the jacobian_product method, which gets the jacobian product matrix with respect to all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.0, 4.0, 6.0], [3.0, 12.0, 27.0]]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x, y: x**2 - y**3\n",
    "h = lambda x, y: x**3 + y**3\n",
    "function_vector = [f, h]\n",
    "\n",
    "jp_object = JacobianProduct(function_vector)\n",
    "inputs = [[1, 2, 3], 0]\n",
    "\n",
    "# getting partial with respect to x (position 0 in lambdas)\n",
    "partial_wrt_x = jp_object.partial(wrt=0, inputs=inputs)\n",
    "print(partial_wrt_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 2., -3.],\n",
      "       [ 3.,  3.]]), array([[  4., -12.],\n",
      "       [ 12.,  12.]]), array([[  6., -27.],\n",
      "       [ 27.,  27.]])]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x, y: x**2 - y**3\n",
    "h = lambda x, y: x**3 + y**3\n",
    "function_vector = [f, h]\n",
    "jp_object = JacobianProduct(function_vector)\n",
    "# inputs are x = {1, 2, 3} and y = {1, 2, 3}\n",
    "# this means we will calculate derivatives for\n",
    "# (1, 1), (2, 2), and (3, 3)\n",
    "inputs = [[1, 2, 3], [1, 2, 3]]\n",
    "\n",
    "# getting jp matrix with respect to all variables\n",
    "jp_matrix = jp_object.jacobian_product(inputs)\n",
    "matrix_1 = np.array([[2,-3],[3, 3]])\n",
    "matrix_2 = np.array([[4, -12],[12, 12]])\n",
    "matrix_3 = np.array([[6, -27],[27, 27]])\n",
    "print(jp_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software organization\n",
    "\n",
    "Our implentation philosphy was to create an object such that all the elemental functions and constants could be arbritrarily combined and nested for differentiation.\n",
    "\n",
    "We created an AD library that contains the following modules:\n",
    "- ```__init__.py``` : Initializes the AD package \n",
    "\n",
    "\n",
    "- ```generic_diff.py``` : Contains GenericDiff class where any specialized classes like a linear, sine, or cos class were reduced to just value and derivative attributes such that we could use our overloaded operators. +, -, /, * , **, and negation were defined according to to the following differentiation rules: chain rule, generalized power rule, quotient rule and product rule. It also overloaded comparison operators, which include <, >, <=, >=, ==, and !=.\n",
    "\n",
    "\n",
    "- ```elemental_functions.py```: Specialized classes in the following modules that inherited from GenericDiff- exponential, sin, cos, tan, sinh, cosh, tanh, acos, asin, atan, exp, log, logit, sqrt classes were defined here. \n",
    "\n",
    "\n",
    "- ```vector_jacobian.py```: A class to handle taking in a vector of autodiff objects and producing a jacobian product matrix\n",
    "\n",
    "We created the following to our generic_diff, elemental_functions, and vector_jacobian modules for all methods and error handling:\n",
    "\n",
    "- ```test_elemental_functions.py```\n",
    "\n",
    "- ```test_generic_diff.py```\n",
    "\n",
    "- ```test_generic_diff_comparisons.py```\n",
    "\n",
    "- ```test_vector_jacobian.py```\n",
    "\n",
    "The test suite is run through pytest and called through Travis CI using the travis yml file - it sends a report to codecov for code coverage.\n",
    "\n",
    "The driver script is:\n",
    "\n",
    "- ```driver_root_solving.py``` This solves for roots using our package according to newton's method\n",
    "\n",
    "Installation:\n",
    "\n",
    "In this iteration, the user is given instructions to download the library through github and create a virtual environment directly. In the final iteration we will be building the package and uploading to Anaconda cloud for distribution. How this is done is by first installing the anaconda client in a local machine, building the package using ```conda build``` from files cloned from the repository and then uploading to an anaconda path defined by our team's anaconda login. We plan to follow the instructions here: https://docs.anaconda.com/anaconda-cloud/user-guide/tasks/work-with-packages/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We wanted to keep our implementation lightweight. Therefore, we do not use any imports except python's standard math library for our single function, single input use case. In our jacobian vector product use case, we will use numpy.\n",
    "\n",
    "The core data structure is our GenericDiff class which has at its base, a scalar value attribute and a scalar derivative attribute. This can be combined or taken in by a more specialized class that differentiates specialized mathematical functions like sine, cosine, exp, etc. We have explained how this works based on the software organization above. Given this data structure, we can arbritrarily combine objects to represent extremely complex mathematical functions.\n",
    "\n",
    "In our jacobian product matrix we will utilize arrays to take care of multiple inputs.\n",
    "\n",
    "Our core classes are:\n",
    "- generic_diff: these are our generic class where we overloaded our multiplication, addition, subtraction, power and division methods)\n",
    "- elemental_functions: this class inherits from the more generic classes above and deals with differentiating sine, cosine, tan, exp, etc.\n",
    "\n",
    "The important attributes in our classes are:\n",
    ".val = value\n",
    ".der = derivative\n",
    "\n",
    "The external dependencies are:\n",
    "- math\n",
    "- numpy\n",
    "\n",
    "Elementary functions covered: \n",
    "\n",
    "- sin\n",
    "- cos\n",
    "- tan\n",
    "- sinh\n",
    "- cosh\n",
    "- tanh\n",
    "- acos\n",
    "- asin\n",
    "- atan\n",
    "- log\n",
    "- logit\n",
    "- sqrt\n",
    "- exp\n",
    "- powers\n",
    "- multiplication\n",
    "- division\n",
    "- addition\n",
    "- subtraction\n",
    "- negation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future features (NOTE: not updated yet!)\n",
    "\n",
    "==========\n",
    "[Milestone 2 version] \n",
    "Next, we’d like to expand on what we currently have by implementing rna_velocity, a new module that can help us better understand RNA velocity, or the time derivative of the gene expression state, which can be estimated by distinguishing between the spliced and unspliced mRNAs products from single cell RNA sequencing. These are also our $s$ and $u$ variables in the differential equation that calculates RNA velocity, respectively. Motivated by the recent paper, RNA velocity of single cells (Manno, et. al), we’re interested in this topic as the study of RNA velocity can give us important insight in predicting the future state of single cells on an hours time- scale, which further serves as a tool in analyzing time dependent phenomena such as tissue regeneration, and more broadly drive advances in developmental lineages and cellular dynamics.\n",
    "The differential equation that calculates RNA velocity is:\n",
    "\n",
    "\\begin{align}\n",
    "\\dot{u} & = \\alpha  - \\beta  u \\\\\n",
    "\\dot{s} & = u - \\gamma s \\\\\n",
    "\\end{align}\n",
    "\n",
    "where  $ \\alpha $ is the time dependent rate of transcription, $\\beta$ the rate of splicing, which will be set to 1 in order to measure all rates in units of the splicing rate, and $ \\gamma $, the rate of degradation. \n",
    "We plan to have our module RNA_velocity call on our existing AutoDiff package and produce plots for RNA velocity. To allow for user interaction, we also plan to build a user friendly interface where the user will be able to input their chosen parameters $\\alpha$ and $\\gamma$ for their desired plot. There will also be the option to download the plot. The only constraint on the parameters chosen by the user is that it should be a real integer or float. ===============\n",
    "\n",
    "\n",
    "\n",
    "- local minima (try to get global minima)\n",
    "- momentum\n",
    "- dropoff to prevent overfitting\n",
    "- gpu, to get rid of floating point errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
